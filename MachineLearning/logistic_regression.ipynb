{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a26b53c",
   "metadata": {},
   "source": [
    "# 1. 逻辑回归 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02060a7",
   "metadata": {},
   "source": [
    "## 1.1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11ac87",
   "metadata": {},
   "source": [
    "逻辑回归假设数据服从伯努利分布，通过极大似然函数的方法，通过梯度下降来求解参数，来达到将数据二分类的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c92c86",
   "metadata": {},
   "source": [
    "## 1.2 基本假设"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc10aeb",
   "metadata": {},
   "source": [
    "* 逻辑回归假设数据服从伯努利分布(Bernoulli Distribution)。\n",
    "  \n",
    "那么根据伯努利分布，如果样本为正例的概率为$p$，样本为负例的概率则为$1-p$。在逻辑回归这个模型里假设$h_{\\theta}(x)$为样本为正例的概率，$1-h_{\\theta}(x)$为样本为负的概率。那么模型可以描述为：\n",
    "$$h_{\\theta}(x;\\theta)=p$$\n",
    "\n",
    "* 逻辑回归假设样本为正的概率服从Logistic函数，即：\n",
    "$$p=\\frac{1}{1+e^{-\\theta^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746f093",
   "metadata": {},
   "source": [
    "所以逻辑回归的最终形式为：\n",
    "$$h_{\\theta}(x;\\theta)=\\frac{1}{1+e^{-\\theta^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884cf26",
   "metadata": {},
   "source": [
    "## 1.3 Logistic函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37d774",
   "metadata": {},
   "source": [
    "### 1.3.1 发生比 (Odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74264499",
   "metadata": {},
   "source": [
    "在统计和概率论中，一个事件的发生比是指改时间发生和不发生之间的比率。假设某随机事件发生的概率是$p$，那么此事件不发生的概率为$1-p$。事件的发生比定义是发生的概率处于不发生的概率，那么用函数$odds(p)$来表示发生比，则：\n",
    "\n",
    "$$odds(p)=\\frac{p}{1-p}$$\n",
    "\n",
    "Note: \n",
    "* 函数$odds(p)$是关于$p$的递增函数(Increasing Function)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2af74",
   "metadata": {},
   "source": [
    "### 1.3.2 Logit函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe5f88",
   "metadata": {},
   "source": [
    "Logit函数就是发生比的对数(logs of odds)，那么用函数$logit(p)$来表示此函数，即\n",
    "\n",
    "$$logit(p)=\\log(\\frac{p}{1-p})$$\n",
    "\n",
    "Note:\n",
    "* Logit函数中的$p$代表正样本个数占全部样本个数的比例/概率。\n",
    "* Logit函数是关于$p$的递增函数(Increasing Function)。\n",
    "* 胜算比(Odds Ratio)是两个概率$p_1$和$p_2$的发生比相除，等价两个概率的logit函数相减，即$$\\text{odds ratio} = \\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}}=\\log(\\frac{p_1}{1-p_1})-\\log(\\frac{p_2}{1-p_2})=logit(p_1)-logit(p_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0c96a",
   "metadata": {},
   "source": [
    "![alternative text](pic/logit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323da65a",
   "metadata": {},
   "source": [
    "### 1.3.3 对数几率函数 (Logistic Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5992f17c",
   "metadata": {},
   "source": [
    "对数几率函数就是Logit函数的逆函数，为Sigmoid函数的一种。\n",
    "\n",
    "* Logistic函数的取值范围是$(0,1)$，所以可用于二分类问题。\n",
    "* 推倒过程如下：\n",
    "\n",
    "    我们从一个非连续、无界的、线性的函数开始，即\n",
    "\n",
    "    $$y=\\theta^Tx=\\theta_0+\\theta_1x+...+\\theta_nx$$\n",
    "\n",
    "    我们将上述函数转化成一个连续的、无界的、线性的函数，即\n",
    "\n",
    "    $$p(x)=\\theta^Tx=p(\\theta_0+\\theta_1x+...+\\theta_nx)$$\n",
    "\n",
    "    如p代表正样本个数占全部样本个数的比例/概率，那么可以套上Logit函数，一个连续的，非线性的，有界的函数，即\n",
    "\n",
    "    $$logit(p) = ln(\\frac{p(x)}{1-p(x)})=\\theta^Tx$$ \n",
    "\n",
    "    我们可以通过取Logit函数的逆函数得到Logistic函数，一个连续的，非线性的，无界的函数，即\n",
    "\n",
    "    $$ln(\\frac{p(x)}{1-p(x)})=\\theta^Tx$$\n",
    "\n",
    "    $$\\frac{p}{1-p}=e^{\\theta^Tx}$$\n",
    "\n",
    "    $$p=e^{\\theta^Tx}(1-p)$$\n",
    "\n",
    "    $$p=e^{\\theta^Tx}-e^{\\theta^Tx}p$$\n",
    "\n",
    "    $$p+e^{\\theta^Tx}p=e^{\\theta^Tx}$$\n",
    "\n",
    "    $$p(1+e^{\\theta^Tx})=e^{\\theta^Tx}$$\n",
    "\n",
    "    $$p=\\frac{e^{\\theta^Tx}}{1+e^{\\theta^Tx}}$$\n",
    "\n",
    "    $$p=\\frac{1}{1+e^{-\\theta^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ec8d2",
   "metadata": {},
   "source": [
    "![alternative text](pic/logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88f7d1",
   "metadata": {},
   "source": [
    "### 1.3.4 Logistic函数/Sigmoid函数的优点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da3b7b",
   "metadata": {},
   "source": [
    "* 线性模型的输出都是在$[-∞,+∞]$之间的，而Sigmoid能够把它映射到$[0,1]$之间。正好这个是概率的范围。\n",
    "* Sigmoid是连续光滑的。\n",
    "* 根据Sigmoid函数，最后推导下来逻辑回归其实就是最大熵模型，根据最大似然估计得到的模型的损失函数就是logloss。这让整个逻辑回归都有理可据。\n",
    "* Sigmoid也让逻辑回归的损失函数成为凸函数，这也是很好的性质。\n",
    "* 逻辑回归的损失函数是二元分类的良好代理函数，这个也是Sigmoid的功劳。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d7f17",
   "metadata": {},
   "source": [
    "## 1.4 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13994f7",
   "metadata": {},
   "source": [
    "### 1.4.1 极大似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02671fc6",
   "metadata": {},
   "source": [
    "逻辑回归的损失函数就是它的极大似然函数(Maximum Likelihood Function)。\n",
    "\n",
    "我们已知逻辑回归中的$p$代表样本为正的概率，即\n",
    "\n",
    "$$P(Y=1|x)=p(x)=h_{\\theta}(x;\\theta)$$\n",
    "$$P(Y=0|x)=1-p(x)=1-h_{\\theta}(x;\\theta)$$\n",
    "\n",
    "似然函数的对于伯努利分布的计算公式为\n",
    "\n",
    "$$L_\\theta(y|x) = P_\\theta(y_i|x_i)$$\n",
    "$$L_\\theta(y|x_1,...,y|x_m,\\theta) = \\prod_{i=1}^{m}P(y_i|x_i,\\theta)$$\n",
    "\n",
    "那么似然函数则为\n",
    "\n",
    "$$L_\\theta(y|x)=\\prod_{i=1}^{m}P(Y=1|x_i)^{y_i}(1-P(Y=1|x_i))^{1-y_i}$$\n",
    "\n",
    "$$L_\\theta(y|x)=\\prod_{i=1}^{m}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}$$\n",
    "\n",
    "$$L_\\theta(y|x)=\\prod_{i=1}^{m}h_\\theta(x_i;\\theta)^{y_i}(1-h\\theta(x_i;\\theta))^{1-y_i}$$\n",
    "\n",
    "以上三个似然函数均一致，只是表达方法不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad5a08",
   "metadata": {},
   "source": [
    "### 1.4.2 交叉熵损失函数 (Cross Entropy Loss Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f64fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe05a67",
   "metadata": {},
   "source": [
    "## 1.5 逻辑回归求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d630d",
   "metadata": {},
   "source": [
    "由于逻辑回归的极大似然函数无法直接求解，所以一般通过对该函数进行梯度下降来不断逼急最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94bacc",
   "metadata": {},
   "source": [
    "我们从已有似然函数开始\n",
    "\n",
    "$$L_\\theta(y|x)=\\prod_{i=1}^{m}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}$$\n",
    "\n",
    "为方便求解，等式两边取对数，写成对数似然函数 (Log Likelihood Function)\n",
    "  \n",
    "$$l_\\theta(y|x)=\\ln L_\\theta(y|x)$$\n",
    "  \n",
    "$$=\\sum \\ln P(y_i|x_i,\\theta)$$\n",
    "  \n",
    "$$=\\sum_{i=1}^{m}y_i\\ln p(x_i)+(1-y_i)\\log(1-p(x_i))$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\\frac{\\partial p(x_i)}{\\partial \\theta}$$\n",
    "\n",
    "$$= \\frac{-1}{(1+e^{-\\theta^Tx})^2} (1+e^{-\\theta^Tx}) $$\n",
    "\n",
    "$$=\\frac{e^{-\\theta^Tx}x}{(1+e^{-\\theta^Tx})^2}$$\n",
    "\n",
    "$$=\\frac{1}{1+e^{-\\theta^Tx}}\\cdot \\frac{e^{-\\theta^Tx}}{1+e^{-\\theta^Tx}} \\cdot x$$\n",
    "\n",
    "$$=p(x)(1-p(x))x$$\n",
    "\n",
    "所以\n",
    "\n",
    "$$\\frac{\\partial l(\\theta)}{\\partial \\theta}$$\n",
    "\n",
    "$$=y\\cdot \\frac{1}{p(x)}\\cdot p(x)\\cdot (1-p(x))\\cdot x+(1-y)\\cdot \\frac{1}{1-p(x)}\\cdot (-1)\\cdot p(x)\\cdot (1-p(x))$$\n",
    "\n",
    "$$=(y-p(x))\\cdot x$$\n",
    "\n",
    "最终，梯度下降通过$l(\\theta)$ 对$\\theta$的一阶导数来找下降方向，并且以迭代的方式来更新参数，更新方式为 :\n",
    "\n",
    "$$g_i = \\frac{\\partial l(\\theta)}{\\partial \\theta_i}=(y_i-p(x_i))x_i$$\n",
    "\n",
    "$$\\theta_i^{k+1}=\\theta_i^k-\\alpha g_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00478e",
   "metadata": {},
   "source": [
    "## 1.6 逻辑回归如何进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c43d1",
   "metadata": {},
   "source": [
    "逻辑回归的分类做法是划定一个阈值 (threshold)，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择，一般会选择0.5做为阈值来划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b38558",
   "metadata": {},
   "source": [
    "## 1.7 逻辑回归的评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c54eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20b5a4",
   "metadata": {},
   "source": [
    "## 1.8 特征与标签的问题及其影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2bdd8",
   "metadata": {},
   "source": [
    "### 1.8.1 高相关度特征/多个特征重复的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9f074",
   "metadata": {},
   "source": [
    "* 如果在损失函数最终收敛的情况下，特征高度相关不会影响分类器的效果。\n",
    "* 但对特征本身来说，假设只有一个特征，将它重复 100 遍训练以后，数据不变，只是这个特征本身重复了100遍。实质上是将原来的特征分成了100份，每个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，训练收敛完以后，可以认为这100个特征和原来那1个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d1f3a",
   "metadata": {},
   "source": [
    "### 1.8.2 去除高相关度特征的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f42062",
   "metadata": {},
   "source": [
    "* 去掉高度相关的特征会让模型的可解释性更好\n",
    "* 可以提高训练的速度\n",
    "    * 特征多会增大训练的时间\n",
    "    * 如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d0088",
   "metadata": {},
   "source": [
    "### 1.8.3 特征离散化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3a309",
   "metadata": {},
   "source": [
    "在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化后交给逻辑回归模型。这样做的优势有以下几点：\n",
    "\n",
    "* 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展(scalable)。\n",
    "* 离散化后的特征对异常数据有很强的鲁棒性(robustness)，防止异常值/离群值对模型的过度影响，减少过拟合风险，不容易受噪声影响，增加模型泛化能力。\n",
    "* 逻辑回归属于广义线性模型，表达能力受限。单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。\n",
    "* 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea597ff",
   "metadata": {},
   "source": [
    "### 1.8.4 特征交叉组合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18673bdb",
   "metadata": {},
   "source": [
    "* 逻辑回归模型属于线性模型，线性模型不能很好处理非线性特征，特征组合可以引入非线性特征，提升模型的表达能力。\n",
    "* 基本特征可以认为是全局建模；组合特征更加精细，是个性化建模。但如果对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸、过拟合。所以基本特征+特征组合兼顾了全局和个性化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77337f46",
   "metadata": {},
   "source": [
    "### 1.8.5 不平衡标签的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650a903",
   "metadata": {},
   "source": [
    "## 1.9 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e91c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a60d4",
   "metadata": {},
   "source": [
    "## 1.10 并行化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996eef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b6c2e",
   "metadata": {},
   "source": [
    "## 1.11 逻辑回归的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafa1cf",
   "metadata": {},
   "source": [
    "**优点**\n",
    "\n",
    "* 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那这个特征最后对结果的影响会比较大。\n",
    "* 逻辑回归的对率函数是任意阶可导函数，数学性质好，易于优化。\n",
    "* 逻辑回归不仅可以预测类别，还可以得到近似的概率预测。\n",
    "* 模型效果不错，在工程上作为baseline是可以接受的。如果特征工程做得好，效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。\n",
    "* 训练速度较快。分类的时候，计算量仅仅只和特征的数目有关，运行时间为$O(d)$。逻辑回归的分布式 (map reduce) 优化sgd发展比较成熟，训练的速度可以通过堆 (clusters) 机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。\n",
    "* 方便输出结果调整。逻辑回归可以很方便的得到最后的分类效果，因为输出的是每个样本的概率分数，可以容易地对这些概率分数进行阈值划分。\n",
    "\n",
    "**缺点**\n",
    "\n",
    "* 准确率不是很高。非常类似线性模型所以形式简单，难以拟合数据的真实分布。\n",
    "* 很难处理数据不平衡问题。\n",
    "* 处理非线性数据较麻烦，逻辑回归不引入其他方法的情况下，只能处理线性可分的数据。\n",
    "* 逻辑回归本身无法筛选特征，需要用会用gbdt来筛选特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da5831",
   "metadata": {},
   "source": [
    "## 1.12 逻辑回归应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f40cd",
   "metadata": {},
   "source": [
    "* CTR预估/推荐系统的learning to rank/各种分类场景。\n",
    "* 某搜索引擎厂的广告CTR预估基线版是LR。\n",
    "* 某电商搜索排序/广告CTR预估基线版是LR。\n",
    "* 某电商的购物搭配推荐用了大量LR。\n",
    "* 某现在一天广告赚1000w+的新闻app排序基线是LR。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8408a",
   "metadata": {},
   "source": [
    "# 2. 逻辑回归与其他模型的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3839660",
   "metadata": {},
   "source": [
    "## 2.1 逻辑回归与线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9adab9",
   "metadata": {},
   "source": [
    "**相同点**\n",
    "\n",
    "* 逻辑回归和线性回归首先都是广义的线性回归\n",
    "\n",
    "**不同点**\n",
    "* 狭义来说逻辑回归逻辑回归引入了Sigmod函数，使样本映射到$[0,1]$之间的数值，是非线性模型。\n",
    "* 逻辑回归是假设变量服从伯努利分布，线性回归假设变量的因变量服从正态分布。\n",
    "* 逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。\n",
    "* 损失函数不同。逻辑回归是交叉熵损失/对数损失函数，线性回归是MSE。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38df14f",
   "metadata": {},
   "source": [
    "## 2.2 逻辑回归与svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1798a33",
   "metadata": {},
   "source": [
    "**相同点**\n",
    "\n",
    "* 都是有监督分类方法\n",
    "* svm不用核函数的情况，都是线性分类方法\n",
    "\n",
    "**不同点**\n",
    "\n",
    "* 损失函数不同。逻辑回归是交叉熵损失/对数损失函数，svm是Hinge loss，最大化函数间隔。\n",
    "* 逻辑回归受数据分布影响，样本不均衡时影响较大，svm不直接依赖于分布。\n",
    "* 逻辑回归可以产生概率，svm不能。\n",
    "* 逻辑回归是经验风险最小化，需要另外加正则，svm自带结构风险最小化不需要加正则项。\n",
    "* 逻辑回归决策考虑所有样本点，svm决策仅仅取决于支持向量。\n",
    "* 逻辑回归每两个点之间都要做内积，而svm只需要计算样本和支持向量的内积，计算量更小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22130086",
   "metadata": {},
   "source": [
    "# 3. 逻辑回归多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e9ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
