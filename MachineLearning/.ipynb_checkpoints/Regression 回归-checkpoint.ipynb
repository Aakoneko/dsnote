{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad54f6aa",
   "metadata": {},
   "source": [
    "# 1. 线性回归 (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb88136",
   "metadata": {},
   "source": [
    "## 1.1 介绍线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b68ec",
   "metadata": {},
   "source": [
    "线性回归是利用线性回归方程的最小平方函数对一个或多个自变量和因变量之间的关系进行建模的一种回归分析。这种函数是一个或多个成为回归系数的模型参数的线性组合。\n",
    "* 简单回归(Simple Linear Regression)：只有一个自变量的情况\n",
    "* 多元回归(Multiple Regression)：大于一个自变量的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df948202",
   "metadata": {},
   "source": [
    "## 1.2 线性回归方程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b8150",
   "metadata": {},
   "source": [
    "$$\\hat y_i = \\theta_0 + \\theta_i \\cdot x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1549e18",
   "metadata": {},
   "source": [
    "* 一元线性回归：$\\hat y = \\theta_0 + \\theta_1 \\cdot x$\n",
    "* 多元线性回归：$\\hat y = \\theta_0 + \\theta_1 \\cdot x_1+\\theta_2 \\cdot x_2+...+\\theta_n \\cdot x_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0787c",
   "metadata": {},
   "source": [
    "## 1.3 线性回归的假设"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a75c0",
   "metadata": {},
   "source": [
    "$$\\mathcal{E}_i = y_i - \\hat y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00911d",
   "metadata": {},
   "source": [
    "* **残差服从正态分布：**\n",
    "\n",
    "残差的方差无偏估计(unbiased variance estimator)是$\\frac{SSE}{n-p}$。如果误差项不呈正态分布，意味着置信区间会变得很不稳定，我们往往需要重点关注一些异常的点，来得到更好的模型。\n",
    "\n",
    "* **残差项相互独立：**\n",
    "\n",
    "若不满足，我们称该模型有自相关性(autocorrelation)，这种情况常发生于时间序列数据集上，后项会受到前项的影响。当自相关性发生的时候，我们测得的标准差往往会偏小，进而会导致置信区间变窄。 可以通过用DW-test来观察。\n",
    "\n",
    "* **残差的方差为常数：**\n",
    "\n",
    "这种特性又叫同方差性(Homoskedasticity)。若不满足，我们称模型具有异方差性(Heteroskedasticity)，误差项的方差不恒定，这常常出现在有异常值（Outlier）的数据集上。\n",
    "\n",
    "* **特征和标签满足线性关系：**\n",
    "\n",
    "若不滿足，模型将无法很好的描述变量之间的关系，极有可能导致很大的泛化误差(generalization error)。如不服从，可以对数据进行非线性转换。\n",
    "\n",
    "* **自变量相互独立(难实现)：**\n",
    "\n",
    "若不满足，我们称模型具有多重共线性(Multicollinearity)，变量之间的联动关系会导致我们测得的方差偏大，置信区间变宽。可以从自变量的散点图或方差膨胀系数(Variance Inflation Factor; VIF)观察。可以采用岭回归，Lasso回归或弹性网(ElasticNet)回归可以一定程度上减少方差，解决多重共线性性问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b67331",
   "metadata": {},
   "source": [
    "## 1.4 线性回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d04f3",
   "metadata": {},
   "source": [
    "$$ L = \\frac{1}{m}\\sum^m_{i=1}(y_i-\\hat y_i^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05d773",
   "metadata": {},
   "source": [
    "线性回归的损失函数是最小二乘法（Ordinary Least Square）/均方誤差（Mean Squared Error）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022afe56",
   "metadata": {},
   "source": [
    "推导过程：\n",
    "$$y^{(i)} = \\theta^Tx^{(i)}+\\mathcal{E}^{(i)}$$\n",
    "$$p(\\mathcal{E}^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\mathcal{E}^{(i)})^2}{2\\sigma^2})$$\n",
    "$$p(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$$\n",
    "$$\\log L(\\theta) = log\\prod^m_{i=1}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$$\n",
    "$$ = \\sum^m_{i=1}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$$\n",
    "$$ = m\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}\\cdot\\frac{1}{2}\\sum^m_{i=1}(y^{(i)}-\\theta^Tx^{(i)})^2$$\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum^m_{i=1}(y^{(i)}-\\theta^Tx^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f543e30",
   "metadata": {},
   "source": [
    "## 1.5 线性回归的求解方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9df573",
   "metadata": {},
   "source": [
    "### 1.5.1 最小二乘法(Ordinary Least Square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e97f8d",
   "metadata": {},
   "source": [
    "线性回归的求解方法使用的是最小二乘法，所谓就小二乘法就是通过最小化均方误差(MSE)来求解模型的权重系数。\n",
    "\n",
    "若要最小化损失函数，即可转化为对其求导，使导数为0，即可得到$\\theta$的解析解。因此可定义目标函数$J(\\theta)=argminL(\\theta)$，并找到一个$\\theta$使损失函数$L(\\theta)$的值最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff8de6",
   "metadata": {},
   "source": [
    "求解过程：\n",
    "$$f(x^{(i)}) = \\frac{1}{2}\\theta^Tx^{(i)}$$\n",
    "$$L(\\theta) = \\frac{1}{2}\\sum^m_{i=1}(y^{(i)}-\\theta^Tx^{(i)})^2$$\n",
    "$$L(\\theta) = \\frac{1}{2}(X\\theta -y)^T(X\\theta -y)$$\n",
    "$$L(\\theta) = \\frac{1}{2}||X\\theta -y||^2_2$$\n",
    "$$\\frac{\\partial}{\\partial\\theta}L(\\theta) = X^T(X\\theta-y)=0$$\n",
    "$$X^T(X\\theta-y)=0$$\n",
    "$$X^TX\\theta-X^Ty=0$$\n",
    "$$X^TX\\theta=X^Ty$$\n",
    "$$(X^TX)^{-1}X^TX\\theta=(X^TX)^{-1}X^Ty$$\n",
    "$$\\theta=(X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a87ec",
   "metadata": {},
   "source": [
    "问题：\n",
    "\n",
    "* 因为计算中$(X^TX)^{-1}$涉及到求逆矩阵的操作，则$X^TX$必须是满秩矩阵(full rank matrix)，而实际情况常常不满足此条件，因此无解。\n",
    "* 矩阵的转置和求逆都是很耗费时间和空间的，如果特征维度大，而求逆过程本身时间复杂度为$O(n^3)$，这种情况求逆更会让计算量过大，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778661c",
   "metadata": {},
   "source": [
    "### 1.5.2 梯度下降法(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbd724",
   "metadata": {},
   "source": [
    "梯度下降法的计算过程就是沿梯度下降的方向求解极小值,进行多次迭代。其中$\\theta$为模型的可学习参数，$J(\\theta)$为目标函数，$\\alpha$为学习率。\n",
    "\n",
    "批量梯度下降法(Batch Gradient Descent)就是使用整个训练集去计算目标函数的梯度。由于每次使用全部数据来计算梯度去更新参数，速度会很慢，而且数据很难一次性全部载入内存当中进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ff080",
   "metadata": {},
   "source": [
    "求解过程：\n",
    "\n",
    "首先定义单个样本的损失函数：\n",
    "$$l^{(i)}(\\theta) = \\frac{1}{2}(\\theta^Tx^{(i)}-y^{(i)})^2$$\n",
    "\n",
    "采用全部训练集数据后的梯度下降法如下($N$为训练集样本的总个数，$j$为迭代次数）：\n",
    "$$\\theta_{j+1}=\\theta_j-\\frac{\\alpha}{N}\\sum^N_{i=1}\\frac{\\partial}{\\partial\\theta_j}l^{(i)}(\\theta_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f641a",
   "metadata": {},
   "source": [
    "## 1.6 线性回归的指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467caed8",
   "metadata": {},
   "source": [
    "### 1.6.1 平均绝对误差 (Mean Absolute Error, MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15ad86",
   "metadata": {},
   "source": [
    "平均绝对误差，又被称为L1范数损失。 \n",
    "$$ MAE=\\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat{y}_i|$$ \n",
    "* 优点：MAE虽能较好衡量回归模型的好坏。\n",
    "* 缺点：绝对值的存在导致函数不光滑，在某些点上不能求导，可以考虑将绝对值改为残差的平方，这就是均方误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4fe2f6",
   "metadata": {},
   "source": [
    "### 1.6.2 均方误差 (Mean Squared Error, MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1d8f4",
   "metadata": {},
   "source": [
    "均方误差，又被称为L2范数损失。 \n",
    "$$ MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$ \n",
    "\n",
    "* 优点：均方误差表示了残差的异动(variance)，且均可以求导。\n",
    "* 缺点：由于MSE与我们的目标变量的量纲不一致，为了保证量纲一致性，我们需要对MSE进行开方 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3289",
   "metadata": {},
   "source": [
    "残差的方差无偏估计(Unbiased Estimator of Variance of Noise/Error)：\n",
    "\n",
    "$$MSE = \\sigma^2 = \\frac{SSE}{n-p}=\\frac{1}{n-p}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\text{ where }p={特征数}$$\n",
    "\n",
    "残差的标准差估计(Estimator of Standard Deviaiton of Noise/Error):\n",
    "$$RMSE = \\sigma = \\sqrt{MSE}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e39672",
   "metadata": {},
   "source": [
    "### 1.6.3 均方跟误差(Root Mean Squared Error, RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6c22d",
   "metadata": {},
   "source": [
    "$$ RMSE=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bace8ed",
   "metadata": {},
   "source": [
    "### 1.6.4 决定系数($R^2$-score; Coefficient of Determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c4447",
   "metadata": {},
   "source": [
    "$R^2$表示反应因变量的变异(variance)中可由数学模型被自变量解释的比例,$R^2$越大，模型准确率越好。\n",
    "\n",
    "$$R^2=可解释性变异占比 = 1-\\frac{不可解释性变异(noise)}{整体变异(variance)}$$\n",
    "$$ R^2=1-\\frac{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\bar{y}_i)^2}=\\frac{MSE}{Var}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e28af2",
   "metadata": {},
   "source": [
    "方差分解：\n",
    "\n",
    "$$SST=SSE+SSR$$\n",
    "\n",
    "* 总平方和(Sum of Squared Total, SST)\n",
    "$$SST = \\sum^n_{i=1}(y_i-\\bar y)^2=SS_{YY}$$\n",
    "\n",
    "* 回归平方和(Sum of Squared Errors, SSE)\n",
    "$$SSE = \\sum^n_{i=1}(y_i-\\hat y)^2=SS_{YY}-\\frac{SS^2_{XY}}{SS_{XX}}$$\n",
    "\n",
    "* 残差平方和(Sum of Squared Regression, SSR)\n",
    "$$SST = \\sum^n_{i=1}(\\hat y_i-\\bar y)^2=\\frac{SS^2_{XY}}{SS_{XX}}$$\n",
    "\n",
    "$$R^2 = 1-\\frac{SSE}{SST} = \\frac{SSR}{SST}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae037fa",
   "metadata": {},
   "source": [
    "Note: \n",
    "* 在现有的模型上，加入新的变量，所得到的$R^2$的总值会增加。\n",
    "* $R^2$的变化趋势对变量是否显著没有直接关系。判断变量是否显著要根据变量对应的权重系数来判断。\n",
    "* 如果新进的特征实在太差，再不济我们也可以让其权重$\\theta_i=0$如果新进的特征x4实在太差，再不济我们也可以让其权重w4=0从而训练出与加入x4之前的模型一样的模型，从而训练出与加入x4之前的模型一样的模型。\n",
    "* 模型越复杂，在训练集上的效果就越好（$R^2$更高），偏差就会越小(low bias)，但同时模型回归系数的方差就会越大(high variance)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d4836",
   "metadata": {},
   "source": [
    "### 1.6.5 偏差(bias)和方差(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea55e5",
   "metadata": {},
   "source": [
    "$$\\text{generalization error} = Bias^2(x)+ Var(x) + \\mathcal{E}$$\n",
    "\n",
    "* 偏差：描述预测模型在不同数据集上训练的模型期望与理论模型之间的差距，偏差越大偏离理论值越远。偏差大可能是underfitting或模型是弱分类器导致的。\n",
    "\n",
    "$$Bias(x)=\\bar f(x)−y $$\n",
    "\n",
    "* 方差：度量了同样大小的训练子集的变动所导致的模型学习性能的变化，即刻画了数据扰动所造成的影响。可以理解为相同规模的不同数据集的模型拟合程度，方差大说明当前数据集拟合的号，换一个数据集就不行了。方差大很有可能是过拟合、训练数据不充足引起的、特征过多引起的。\n",
    "\n",
    "$$Var(x)=E_D[(f(x;D)−\\bar f(x))^2]$$\n",
    "\n",
    "Note:\n",
    "\n",
    "此处符号具有如下意义\n",
    "\n",
    "* $x$ - 测试样本\n",
    "* $D$ - 数据集\n",
    "* $y_D$ - x在数据集中的标记\n",
    "* $y$ - x的真实标记\n",
    "* $f$ - 训练集D学到的模型\n",
    "* $f(x;D)$ - 训练集D学到的模型f对x的输出\n",
    "* $\\bar f(x)$ - 模型f对x的期望输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd2b2e",
   "metadata": {},
   "source": [
    "## 1.7 线性回归如何增强模型表达能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4ba7a",
   "metadata": {},
   "source": [
    "* 连续特征的离散化\n",
    "* 特征交叉\n",
    "* gbdt交叉特征提取\n",
    "* polynimial\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36803db1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f7df5e7",
   "metadata": {},
   "source": [
    "## 1.8 为什么线性回归和逻辑回归要用对特征进行离散化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd76fc",
   "metadata": {},
   "source": [
    "* 离散特征的增加和减少都很容易，易于模型的快速迭代。\n",
    "\n",
    "* 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。\n",
    "\n",
    "* 离散化后的特征对异常数据有很强的鲁棒性(robustness)。如果特征没有离散化，一个异常数据会给模型造成很大的干扰；离散化后异常数据也只对应于一个权重,也不会对预测结果产生影响。\n",
    "\n",
    "* 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。\n",
    "\n",
    "* 离散化后可以进行特征交叉，加入特征A离散化为M个值，特征B离散为N个值，那么交叉之后会有M*N个变量，进一步引入非线性，提升表达能力。\n",
    "\n",
    "* 当使用连续特征时，一个特征对应于一个权重，那么，如果这个特征权重较大，模型就会很依赖于这个特征，这个特征的一个微小变化可能会导致最终结果产生很大的变化，这样子泛化能力差，容易过拟合。而使用离散特征的时候，一个特征变成了多个，权重也变为多个，那么之前连续特征对模型的影响力就被分散弱化了，从而降低了过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20c418",
   "metadata": {},
   "source": [
    "## 1.9 线性回归效果不好的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbed72",
   "metadata": {},
   "source": [
    "线性模型，不通过大量特征工程来将非线性问题转化为线性则效果不好也正常。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ca4c9",
   "metadata": {},
   "source": [
    "## 1.10 线性回归如何降低过拟合的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2f5f3",
   "metadata": {},
   "source": [
    "* L1、L2正则化\n",
    "* 增大样本数\n",
    "* 减少特征的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6a4d0",
   "metadata": {},
   "source": [
    "## 1.11 共线性(Collinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58ce2e",
   "metadata": {},
   "source": [
    "### 1.11.1 共线性的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcef36",
   "metadata": {},
   "source": [
    "统计学中，共线性即多重共线性。多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。完全共线性的情况并不多见，一般出现的是在一定程度上的共线性，即近似共线性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7386f72",
   "metadata": {},
   "source": [
    "### 1.11.2 共线性的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7cc724",
   "metadata": {},
   "source": [
    "太多相关度很高的特征并没有提供太多的信息量，并没有提高数据可以达到的上限，相反，数据集拥有更多的特征意味着更容易收到噪声的影响，更容易收到特征偏移的影响等等。对于线性模型来说，会导致以下问题：\n",
    "\n",
    "* 模型参数估计不具备可解释性：有时甚至会出现回归系数的符号与实际情况完全相反的情况。\n",
    "\n",
    "* 本应该显著的自变量不显著，本不显著的自变量却呈现出显著性\n",
    "\n",
    "* 多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大，影响模型的泛化误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b3762",
   "metadata": {},
   "source": [
    "### 1.11.3 共线性的检验方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e16e7",
   "metadata": {},
   "source": [
    "* 相关性分析：检验变量之间的相关系数R在±0.7或以上\n",
    "* 方差膨胀因子VIF：当VIF大于5或10时，代表模型存在严重的共线性问题；\n",
    "* 条件数检验：当条件数大于100、1000时，代表模型存在严重的共线性问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3b34f",
   "metadata": {},
   "source": [
    "### 1.11.4 共线性的解决方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9079851",
   "metadata": {},
   "source": [
    "* PCA降维\n",
    "* 正则化\n",
    "* 逐步回归法(stepwise regression)：一种常用的消除多重共线性、选取“最优”回归方程的方法。其做法是将逐个引入自变量，引入的条件是该自变量经F检验是显著的，每引入一个自变量后，对已选入的变量进行逐个检验，如果原来引入的变量由于后面变量的引入而变得不再显著，那么就将其剔除。引入一个变量或从回归方程中剔除一个变量，为逐步回归的一步，每一步都要进行F 检验，以确保每次引入新变量之前回归方程中只包含显著的变量。这个过程反复进行，直到既没有不显著的自变量选入回归方程，也没有显著自变量从回归方程中剔除为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac354c",
   "metadata": {},
   "source": [
    "# 2. 岭回归 (Ridge Regression) 和 Lasso回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a24c4",
   "metadata": {},
   "source": [
    "## 2.1 岭回归和Lasso回归的应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83937176",
   "metadata": {},
   "source": [
    "在线性回归模型中，其参数估计公式为$\\beta=(X^TX)^{−1}X^Ty$，当$X$不是满秩时，也就是当特征存在共线性/自变量个数多于样本量时，$X^TX$不可逆时无法求出$\\beta$。另外如果$|X^TX|$越趋近于0，会使得回归系数趋向于无穷大，此时得到的回归系数是无意义的。解决这类问题可以使用岭回归和LASSO回归，主要针对自变量之间存在多重共线性或者自变量个数多于样本量的情况。\n",
    "\n",
    "作用：\n",
    "\n",
    "* 减少模型复杂度\n",
    "* 减少均方误差\n",
    "* 降低方差\n",
    "* 提高模型拟合效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ea077",
   "metadata": {},
   "source": [
    "## 2.2 岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e48626",
   "metadata": {},
   "source": [
    "### 2.2.1 介绍岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968eb370",
   "metadata": {},
   "source": [
    "岭回归模型在线性回归模型的目标函数上加了一个L2范数的惩罚项,可以讲部分特征的回归系数缩减，但不能缩减至0，剔除变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e46cd",
   "metadata": {},
   "source": [
    "### 2.2.2 岭回归的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de27cd",
   "metadata": {},
   "source": [
    "$$J(\\beta) = \\sum(y-X\\beta)^2+\\lambda||\\beta||^2_2$$\n",
    "$$J(\\beta) = \\sum(y-X\\beta)^2+\\sum\\lambda\\beta^2$$\n",
    "\n",
    "* $\\lambda$为非负数\n",
    "* $\\lambda$越大，则为了使$J(\\beta)$最小，回归系数$\\beta$缩得就越小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39d553",
   "metadata": {},
   "source": [
    "### 2.2.3 岭回归的求解方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a74c2",
   "metadata": {},
   "source": [
    "$$J(\\beta) = (y-X\\beta)^T(y-X\\beta) + \\lambda\\beta^T\\beta$$\n",
    "\n",
    "$$J(\\beta) = y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta + \\lambda\\beta^T\\beta$$\n",
    "\n",
    "$$\\text{Let} \\frac{\\partial}{\\partial\\beta}J(\\beta) = 0$$\n",
    "\n",
    "$$0-X^Ty-X^Ty+2X^TX\\beta+2\\lambda\\beta=0$$\n",
    "\n",
    "$$\\beta = (X^TX+\\lambda I)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629747c",
   "metadata": {},
   "source": [
    "* L2范数惩罚项的加入使得$(X^TX+\\lambda I)$满秩，保证了可逆。\n",
    "* 回归系数$\\beta$的估计不再是无偏估计。\n",
    "* 岭回归以放弃无偏性(unbiasness)、降低精确度为代价来解决病态矩阵问题的回归方法。\n",
    "* 岭回归会让偏差变大(higher bias)，方差变小(lower variance)，部分时候模型泛化能力更强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5bc7cf",
   "metadata": {},
   "source": [
    "### 2.2.4 惩罚系数$\\lambda$的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a00fd",
   "metadata": {},
   "source": [
    "对于岭回归的$\\lambda$而言，随着$\\lambda$的增大，$|X^TX+\\lambda I|$就越大，$(X^TX+\\lambda I)^{-1}$就越小，模型的方差就越小；而$\\lambda$越大使得$\\beta$的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的$\\lambda$值来平衡模型的方差和偏差。\n",
    "\n",
    "通常我们使用交叉验证法来选择最优$\\lambda$。K-Fold交叉验证法的思想是，将数据集拆等分分为k个数据组，从k组中挑选k-1组用于模型的训练，剩下的1组用于模型的测试，则会有k-1个训练集和测试集配对。每一种训练集和测试集下都会有对应的一个模型及模型评分,如均方误差(RMSE)，进而可以得到一个平均评分。对于每一个模型，我们需要构造不同的$\\lambda$值，并选择平均评分最优的$\\lambda$值，也就是均方误差最低的$\\lambda$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21298d",
   "metadata": {},
   "source": [
    "## 2.3 Lasso回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce2f93",
   "metadata": {},
   "source": [
    "### 2.3.1 介绍Lasso回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e176c",
   "metadata": {},
   "source": [
    "Lasso回归模型在线性回归模型的目标函数上加了一个L1范数的惩罚项，可以将一些不重要的回归系数缩减为0，达到剔除变量/降维的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084fdf3",
   "metadata": {},
   "source": [
    "### 2.3.2 Lasso回归的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e502e",
   "metadata": {},
   "source": [
    "$$J(\\beta) = \\sum(y-X\\beta)^2+\\lambda||\\beta||_1$$\n",
    "$$J(\\beta) = \\sum(y-X\\beta)^2+\\sum\\lambda｜\\beta｜$$\n",
    "$$J(\\beta) = ESS(\\beta)+\\lambda l_1(\\beta)$$\n",
    "\n",
    "Note:\n",
    "\n",
    "* $ESS(\\beta)$ - 误差平方和\n",
    "* $\\lambda l_1(\\beta)$ - 惩罚项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb4b67",
   "metadata": {},
   "source": [
    "### 2.3.3 Lasso回归的求解方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e1f4d",
   "metadata": {},
   "source": [
    "由于惩罚项变成了绝对值，则在零点处就不可导，故采用坐标下降法：对于p维参数的可微凸函数$J(\\beta)$，如果存在$\\hat\\beta$使得$J(\\beta)$在每个坐标轴上均达到最小值则$\\hat{J(\\beta)} $就是点$\\hat\\beta$上的全局最小值。控制其他p-1个参数不变，对目标函数中的某一个$\\beta_j$求偏导，以此类推对剩下的p-1个参数求偏导，最终令每个分量下的导函数为0，得到使目标函数达到全局最小$\\hat\\beta$。\n",
    "\n",
    "* 先求$ESS(\\beta）$的导数\n",
    "\n",
    "$$ESS(\\beta) = \\sum^n_{i=1}(y_i-\\sum^p_{j=1}\\beta x_{ij})^2$$\n",
    "\n",
    "$$ESS(\\beta) = \\sum^n_{i=1}(y_i^2+(\\sum^p_{j=1}\\beta x_{ij})^2-2y_i(\\sum^p_{j=1}\\beta_jx_{ij}))$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta_j}ESS(\\beta) = -2\\sum^n_{i-1}x_{ij}(y_i-\\sum^p_{j=1}\\beta_jx_{ij})$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta_j}ESS(\\beta) = -2\\sum^n_{i-1}x_{ij}(y_i-\\sum_{k\\neq j}\\beta_kx_{ik}-\\beta_jx_{ij})$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta_j}ESS(\\beta) = -2\\sum^n_{i-1}x_{ij}(y_i-\\sum_{k\\neq j}\\beta_kx_{ik})+2\\beta_j\\sum_{i=1}^nx^2_{ij}$$\n",
    "$$\\frac{\\partial}{\\partial\\beta_j}ESS(\\beta) = -2m_j+2\\beta_jn_j$$\n",
    "\n",
    "* 求惩罚项的次倒数（惩罚项不可导）：\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\frac{\\partial}{\\partial\\beta_j}\\lambda l_1(\\beta)=\n",
    "    \\begin{cases}\n",
    "        \\lambda & \\text{if } \\beta_j>0\\\\\n",
    "        [-\\lambda,\\lambda] & \\text{if } \\beta_j=0\\\\\n",
    "        -\\lambda & \\text{if } \\beta_j<0\n",
    "    \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "* 使两个偏导数相加等于0：\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\frac{\\partial}{\\partial\\beta_j}ESS(\\beta)+\\frac{\\partial}{\\partial\\beta_j}\\lambda l_1(\\beta)=\n",
    "    \\begin{cases}\n",
    "        -2m_j+2\\beta_jn_j+\\lambda & \\text{if } \\beta_j>0\\\\\n",
    "        [-2m_j-\\lambda,-2m_j+\\lambda] & \\text{if } \\beta_j=0\\\\\n",
    "        -2m_j+2\\beta_jn_j-\\lambda & \\text{if } \\beta_j<0\n",
    "    \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\beta_j=\n",
    "    \\begin{cases}\n",
    "        \\frac{(m_j-\\frac{\\lambda}{2})}{n_j}& \\text{if } m_j>\\frac{\\lambda}{2}\\\\\n",
    "        [-2m_j-\\lambda,-2m_j+\\lambda] & \\text{if } m_j\\in[-\\frac{\\lambda}{2},\\frac{\\lambda}{2}]\\\\\n",
    "        \\frac{(m_j+\\frac{\\lambda}{2})}{n_j} & \\text{if } m_j<\\frac{\\lambda}{2}\n",
    "    \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1157532",
   "metadata": {},
   "source": [
    "### 2.3.4 惩罚系数$\\lambda$的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b860ce",
   "metadata": {},
   "source": [
    "和岭回归一样，我们使用交叉验证法来选择最优$\\lambda$。K-Fold交叉验证法的思想是，将数据集拆等分分为k个数据组，从k组中挑选k-1组用于模型的训练，剩下的1组用于模型的测试，则会有k-1个训练集和测试集配对。每一种训练集和测试集下都会有对应的一个模型及模型评分,如均方误差(RMSE)，进而可以得到一个平均评分。对于每一个模型，我们需要构造不同的$\\lambda$值，并选择平均评分最优的$\\lambda$值，也就是均方误差最低的$\\lambda$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936740c5",
   "metadata": {},
   "source": [
    "## 2.4 弹性网络回归（ElasticNet）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73572bca",
   "metadata": {},
   "source": [
    "### 2.4.1 介绍弹性网络回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0784473",
   "metadata": {},
   "source": [
    "ElasticNet回归模型在线性回归模型的目标函数上加了一个L1范数和L2范数的惩罚项。ElasticNet在我们发现用Lasso回归太过，太多特征被稀疏为0；而岭回归也正则化的不够，回归系数衰减太慢的时候，可以考虑使用用于来综合两种情况，得到比较好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a222a",
   "metadata": {},
   "source": [
    "### 2.4.2 弹性网络回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c5172",
   "metadata": {},
   "source": [
    "$$ J(\\theta)=\\frac{1}{2} \\sum_{i}^{m}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}+\\lambda\\left(\\rho \\sum_{j}^{n}\\left|\\theta_{j}\\right|+(1-\\rho) \\sum_{j}^{n} \\theta_{j}^{2}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364aaebc",
   "metadata": {},
   "source": [
    "## 2.5 Ridge和Lasso对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a0492",
   "metadata": {},
   "source": [
    "![alternative text](pic/ridge&lasso1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f4875",
   "metadata": {},
   "source": [
    "![alternative text](pic/ridge&lasso2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7048e96",
   "metadata": {},
   "source": [
    "左边是Lasso对应的误差等位线和正方形限制区域，右边是Ridge对应的等位线和圆形限制区域。上面图中围绕在$\\beta$周围的椭圆表示有相同RSS的参数估计。随着椭圆的扩大，对应的RSS增加。Lasso和Ridge的估计值就是在一定的限制区域下，椭圆不断扩张的过程中和限制区域的第一个接触点。如果有某个参数的估计是0的话，触点一定在某条坐标轴上。由于Ridge的限制区域是圆形，所以真正的触点无法落在坐标轴上，可能无限接近，但就是到不了。所以Ridge无法将参数收缩成0，而Lasso可以。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
